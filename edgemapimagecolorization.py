# -*- coding: utf-8 -*-
"""EdgeMapImageColorization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rck3-LKOdzD13eY22l0dN32BF0TTHI2O
"""

from google.colab import drive
drive.mount('/content/drive')

###################    WE WORRKED ON 64X64 SIZE IMAGES  #####

import torch
import torchvision.datasets
from sklearn.datasets import load_files
from matplotlib.pyplot import imshow
from google.colab.patches import cv2_imshow
import cv2
import joblib
import numpy as np
import os

# ls '/content/drive/MyDrive/flickr30k_images/flickr30k_images' | wc -l

def data_load_preprocess():
  folder_name = '/content/drive/MyDrive/flickr30k_images/flickr30k_images'
  cnt=0
  RGB_image_list=[]
  # gray_image_list=[]
  sal_image_list=[]
  stack_gray_edge_list=[]
  for file in os.listdir(folder_name):
    if file[-1]!='g':
      print(file)
    cnt=cnt+1
    print(cnt)
    if cnt >5000:
      break

    file = cv2.imread(folder_name + '/' + file)
    # print(file)
    file = cv2.resize(file,(64,64))
    
    gray_image=cv2.cvtColor(file,cv2.COLOR_BGR2GRAY)
    edge_image = cv2.Canny(gray_image,100,200,apertureSize=3,L2gradient = True)

    img = (file-128.0)/128.0
    gray_image = (gray_image-128.0)/128.0
    edge_image = edge_image/255
    
    

    saliency = cv2.saliency.StaticSaliencyFineGrained_create()
    (success, saliencyMap) = saliency.computeSaliency(file)
    saliencyMap_tanh = (saliencyMap*2)-1
    saliencyMap_tanh = torch.from_numpy(saliencyMap_tanh)
    # print(saliencyMap_tanh.shape)

    grayimg = torch.from_numpy(gray_image)
    edgeimg = torch.from_numpy(edge_image)
    img = torch.from_numpy(img)
    img = img.permute(2,0,1)
    
    grayimg = grayimg.reshape(1,grayimg.shape[0],grayimg.shape[1])
    edgeimg = edgeimg.reshape(1,edgeimg.shape[0],edgeimg.shape[1])
    stack_gray_edge = torch.cat((grayimg,edgeimg),0)

    saliencyMap_tanh = saliencyMap_tanh.reshape(1,saliencyMap_tanh.shape[0],saliencyMap_tanh.shape[1])

    RGB_image_list.append(img)
    # gray_image_list.append(grayimg)
    stack_gray_edge_list.append(stack_gray_edge)
    sal_image_list.append(saliencyMap_tanh)
  # print(len(RGB_image_list))
  # print(len(gray_image_list))
  return RGB_image_list,stack_gray_edge_list,sal_image_list


# RGB_image,stack_gray_edge,sal_image = data_load_preprocess()

# print(RGB_image[7].shape)

# joblib.dump(sal_image,'/content/drive/MyDrive/DL project/endsem/sal_image')

# joblib.dump(RGB_image,'/content/drive/MyDrive/DL project/endsem/RGB_image')

# joblib.dump(stack_gray_edge,'/content/drive/MyDrive/DL project/endsem/stack_gray_edge')

# joblib.dump(gray_image,'/content/drive/MyDrive/DL project/endsem/gray_image')

sal_image = joblib.load('/content/drive/My Drive/DL project/endsem/sal_image')
# gray_image = joblib.load('/content/drive/My Drive/DL project/endsem/gray_image')
RGB_image = joblib.load('/content/drive/My Drive/DL project/endsem/RGB_image')
stack_gray_edge = joblib.load('/content/drive/MyDrive/DL project/endsem/stack_gray_edge')

print(len(sal_image))

#RGB_image4, gray_image4 = preprocess(train_files,37500,50030)
#RGB_image3, gray_image3 = preprocess(train_files,25000,37500)
#RGB_image2, gray_image2 = preprocess(train_files,12500,25000)
#RGB_image1, gray_image1 = preprocess(train_files,0,12500)
#joblib.dump(RGB_image1,'/content/drive/My Drive/DL project/RGB_image1')
#joblib.dump(RGB_image2,'/content/drive/My Drive/DL project/RGB_image2')
#joblib.dump(RGB_image3,'/content/drive/My Drive/DL project/RGB_image3')
#joblib.dump(RGB_image4,'/content/drive/My Drive/DL project/RGB_image4')

# RGB_image1 = joblib.load('/content/drive/My Drive/DL project/RGB_image1')


#RGB_image2 = joblib.load('/content/drive/My Drive/DL project/RGB_image2')
#RGB_image3 = joblib.load('/content/drive/My Drive/DL project/RGB_image3')
#RGB_image4 = joblib.load('/content/drive/My Drive/DL project/RGB_image4')

# gray_image1 = joblib.load('/content/drive/My Drive/DL project/gray_image1')


#gray_image2 = joblib.load('/content/drive/My Drive/DL project/gray_image2')
#gray_image3 = joblib.load('/content/drive/My Drive/DL project/gray_image3')
#gray_image4 = joblib.load('/content/drive/My Drive/DL project/gray_image4')

# print(len(RGB_image1))

'''RGB_image = RGB_image1 + RGB_image2 + RGB_image3 +RGB_image4
gray_image= gray_image1 + gray_image2 + gray_image3 + gray_image4'''
# RGB_image = RGB_image1[:3000]
# gray_image = gray_image1[:3000]

# print(np.array(RGB_image[2]).shape)
# print(np.array(gray_image[2]).shape)

stack_gray_edge[0].shape

trainset=  list(zip(stack_gray_edge,RGB_image,sal_image))

def output_display_test(output,flag,i,flag2):
    
    output = output.cpu().detach().numpy()
    if flag==0:
        denorm_output = ((output[0][0]+1)/2)* 255
        bgr=denorm_output

    if flag==1:

        denorm_output_1 = ((output[0][0]+1)/2)* 255
        denorm_output_2 = ((output[0][1]+1)/2)* 255
        denorm_output_3 = ((output[0][2]+1)/2)* 255
        bgr = np.dstack((denorm_output_1,denorm_output_2,denorm_output_3))  # stacks 3 h x w arrays -> h x w x 3
    
    cv2_imshow(bgr)
    
    # filename='/content/drive/My Drive/DL project/endsem/output/test/'+'img_no'+str(i)+'_'+flag2+'.jpg'
    # cv2.imwrite(filename,bgr)

def output_display(output,epoch,i,flag):
    output = output.cpu().detach().numpy()
    denorm_output_1 = ((output[0][0]+1)/2)* 255
    denorm_output_2 = ((output[0][1]+1)/2)* 255
    denorm_output_3 = ((output[0][2]+1)/2)* 255
    bgr = np.dstack((denorm_output_1,denorm_output_2,denorm_output_3))  # stacks 3 h x w arrays -> h x w x 3
    cv2_imshow(bgr)

    
    # filename='/content/drive/My Drive/DL project/endsem/output/'+'epoch_no'+str(epoch)+'img_no'+str(i)+'_'+flag+'.jpg'
    # cv2.imwrite(filename,bgr)

import torchvision.models as models
from torchvision.models import resnet18
import torch.nn as nn
import skimage
import skimage.transform
# resnet18 = models.resnet18(pretrained=True)
# res = list(resnet18.children())
# # print(list(res[5][0].children())[-1])
# print(list(resnet18.children())[-2])
# print("btntnrfbs")

class resnet_pretrained(nn.Module):
    def __init__(self, network='resnet18'):
        super(resnet_pretrained, self).__init__()
        self.network = network
        self.net = models.resnet18(pretrained=True)
        
        # for param in self.net.parameters():
        #   param.requires_grad = False

        self.net = nn.Sequential(*list(self.net.children())[:-2])

    def forward(self, x):
        # print("x==>",x)
        # x=x.float()
        # x = skimage.transform.pyramid_expand(x, upscale=3.5, sigma=2)
        x = torchvision.transforms.Resize((224,224))(x)
        x = torch.cat((x,x,x),1)
        # print("after cat resized",x.shape) # 1,3,224,224
        x= self.net(x)
        # print("after net x",x.shape) # 1,512,7,7
        x = torchvision.transforms.Resize((2,2))(x)
        # print("x transform final resize",x.shape)
        # print(x)
        # print("res",x.shape)
        # x = x.permute(0, 2, 3, 1)
        # print("x shape",x.shape)
        return x
# print(RGB_image1[0].shape)
# y = RGB_image1[0].permute(2,0,1)
# y = y.reshape(1,y.shape[0],y.shape[1],y.shape[2])
# enc = Encoder()
# x= enc(y)

def to_device(data, device):
    if isinstance(data, (list,tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)
    
class DeviceDataLoader():
    def __init__(self, dl, device):
        self.dl = dl
        self.device = device
        
    def __iter__(self):
        for b in self.dl: 
            yield to_device(b, self.device)

    def __len__(self):
        return len(self.dl)

from torch.utils.data import DataLoader

batch_size = 64
train_dataset = DataLoader(trainset, batch_size,shuffle=True)
# val_dataset = DataLoader(val_ds,batch_size)
train_dataset = DeviceDataLoader(train_dataset,torch.device('cuda'))
# val_dataset = DeviceDataLoader(val_dataset,torch.device('cuda'))

import torch
import torch.nn as nn
import torch.nn.functional as F

# from network_module import *

# ----------------------------------------
#         Initialize the networks
# ----------------------------------------


def weights_init(m):
  classname = m.__class__.__name__
  if hasattr(m, 'weight') and classname.find('Conv') != -1:
    torch.nn.init.xavier_normal_(m.weight.data, gain=0.02)
  
opt_in_channels= 2
opt_out_channels = 3
opt_start_channels=64
opt_latent_channels= 128
opt_init_type='xavier'
opt_init_gain=0.02    



        
# ----------------------------------------
#                Generator
# ----------------------------------------
# SCGAN's generator
class SCGAN(nn.Module):
    def __init__(self):
        super(SCGAN, self).__init__()

        self.res_obj = resnet_pretrained().cuda()
        self.res_obj.eval()
        
        # Downsample blocks
        self.down1=torch.nn.Conv2d(opt_in_channels, opt_start_channels,                7, 1, 3)  #kernel,stride,padding
        self.down2=torch.nn.Conv2d(opt_start_channels, opt_start_channels * 2,          3, 2, 1)
        self.down3=torch.nn.Conv2d(opt_start_channels * 2, opt_start_channels * 4,       3, 2, 1)
        self.down4=torch.nn.Conv2d(opt_start_channels * 4, opt_start_channels * 8,      3, 2, 1)
        self.down5=torch.nn.Conv2d( opt_start_channels * 8, opt_start_channels * 8,      3, 2, 1)
        self.down6=torch.nn.Conv2d(opt_start_channels * 8 , opt_start_channels * 8 ,     3, 2, 1)
        self.down7=torch.nn.Conv2d(opt_start_channels * 8 , opt_start_channels * 8,      3, 2, 1)
        self.down8=torch.nn.Conv2d(opt_start_channels * 8, opt_start_channels * 8,       3, 2, 1)
        self.down9=torch.nn.Conv2d(opt_start_channels * 8 + 512, opt_start_channels * 8,        3, 2, 1)

        # Fusion & Upsample
        self.up1 = torch.nn.Conv2d(opt_start_channels * 8, opt_start_channels * 8,  3, 1, 1)
        self.up2 = torch.nn.Conv2d(opt_start_channels * 16, opt_start_channels * 8, 3, 1, 1)
        self.up3 = torch.nn.Conv2d(opt_start_channels * 16, opt_start_channels * 8, 3, 1, 1)
        self.up4 = torch.nn.Conv2d(opt_start_channels * 16, opt_start_channels * 8, 3, 1, 1)
        self.up5 = torch.nn.Conv2d(opt_start_channels * 16, opt_start_channels * 8, 3, 1, 1)
        self.up6 = torch.nn.Conv2d(opt_start_channels * 16, opt_start_channels * 4, 3, 1, 1)
        self.up7 = torch.nn.Conv2d(opt_start_channels * 8, opt_start_channels * 2,  3, 1, 1)
        self.up8 = torch.nn.Conv2d(opt_start_channels * 4, opt_start_channels ,      3, 1, 1)
        self.up9 = torch.nn.Conv2d(opt_start_channels * 2, opt_out_channels+1,        3, 1, 1)


    def forward(self, x):
        # print("x===>",x)
        # print("x ka shape==>", x.shape)
        # Mainstream Encoder
        down1 = F.leaky_relu (self.down1(x))                                       # out: batch * 64 * 256 * 256
        # print("down1 ka shape==>", down1.shape)
        down2 = F.leaky_relu (self.down2(down1))                                     # out: batch * 128 * 128 * 128
        # print("down2 ka shape==>", down2.shape)
        down3 = F.leaky_relu (self.down3(down2))                                    # out: batch * 256 * 64 * 64
        # print("down3 ka shape==>", down3.shape)
        down4 =F.leaky_relu (self.down4(down3))                                       # out: batch * 512 * 32 * 32
        # print("down4 ka shape==>", down4.shape)
        down5 = F.leaky_relu (self.down5(down4))                                     # out: batch * 512 * 16 * 16
        # print("down5 ka shape==>", down5.shape)
        down6 = F.leaky_relu (self.down6(down5))                                   # out: batch * 512 * 8 * 8
        # print("down6 ka shape==>", down6.shape)

        
        # print("x shape before res",x.shape )
        res_feat = self.res_obj(x[:,0:1,:,:])
        # print("res_feat shape",res_feat.shape)
        down6_res = torch.cat((down6,res_feat),1)
        # print("down6_after_res",down6_res.shape)
        
        """
        down7 = F.leaky_relu (self.down7(down6))                                    # out: batch * 512 * 4 * 4 (CHECK THIS)
        print("down7 ka shape==>", down7.shape)
        down8 = F.leaky_relu (self.down8(down7))                                       # out: batch * 512 * 2 * 2
        print("down8 ka shape==>", down8.shape)
        """
        
        down9 = F.leaky_relu (self.down9(down6_res)) #chengeed from down 8 to down 6      # out: batch * 512 * 1 * 1
        # print("down9 ka shape==>", down9.shape)

        # print("\n\nDECODER STARTED...")
        
        # Mainstream Decoder
        up1 = F.interpolate(down9, scale_factor = 2, mode = 'nearest')
        up1=  F.leaky_relu(self.up1(up1))                            
        # print("up 1 shape b4 cat==>",up1.shape)
        up1 = torch.cat((down6, up1), 1)                            # out: batch * (1024 = 512 + 512) * 2 * 2
        # print("up 1 shape after cat==>",up1.shape)


        up4 = F.interpolate(up1, scale_factor = 2, mode = 'nearest')   #changed up 3 to up1
        up4=  F.leaky_relu(self.up4(up4))
        # print("up 4 shape b4 cat==>",up4.shape)
        up4 = torch.cat((down5, up4), 1)                            # out: batch * (1024 = 512 + 512) * 2 * 2
        # print("up 4 shape after cat==>",up4.shape)

        up5 = F.interpolate(up4, scale_factor = 2, mode = 'nearest')
        up5=  F.leaky_relu(self.up5(up5))
        # print("up 5 shape b4 cat==>",up5.shape)
        up5 = torch.cat((down4, up5), 1)                            # out: batch * (1024 = 512 + 512) * 2 * 2
        # print("up 5 shape after cat==>",up5.shape)

        up6 = F.interpolate(up5, scale_factor = 2, mode = 'nearest')
        up6=  F.leaky_relu(self.up6(up6))
        # print("up 6 shape b4 cat==>",up6.shape)
        up6 = torch.cat((down3, up6), 1)                            # out: batch * (1024 = 512 + 512) * 2 * 2
        # print("up 6 shape after cat==>",up6.shape)

        up7 = F.interpolate(up6, scale_factor = 2, mode = 'nearest')
        up7=  F.leaky_relu(self.up7(up7))
        # print("up 7 shape b4 cat==>",up7.shape)
        up7 = torch.cat((down2, up7), 1)                            # out: batch * (1024 = 512 + 512) * 2 * 2
        # print("up 7 shape after cat==>",up7.shape)

        up8 = F.interpolate(up7, scale_factor = 2, mode = 'nearest')
        up8=  F.leaky_relu(self.up8(up8))
        # print("up 8 shape b4 cat==>",up8.shape)
        up8 = torch.cat((down1, up8), 1)                            # out: batch * (1024 = 512 + 512) * 2 * 2
        # print("up 8 shape after cat==>",up8.shape)

        # up9 = F.interpolate(up8, scale_factor = 2, mode = 'nearest')
        up9=  F.tanh(self.up9(up8))
        # print("up9 shape",up9.shape)
        up9_rgb = up9[:,:-1,:]
        up9_saliency = up9[:,-2:-1,:]
        # print("up9 rgb shape",up9_rgb.shape)
        # print("up9 sal shape",up9_saliency.shape)
        #print("up 9 shape b4 cat==>",up9.shape)
        # up1 = torch.cat((down8, up1), 1)                            # out: batch * (1024 = 512 + 512) * 2 * 2
        # print("up 1 shape after cat==>",up1.shape)
        # sal = self.attention_prediction_network( up6, up7,up8) 
        return up9_rgb, up9_saliency

# ----------------------------------------
#               Discriminator (chk nicely)
# ----------------------------------------
# This is a kind of PatchGAN. Patch is implied in the output. This is 70 * 70 PatchGAN
class PatchDiscriminator70(nn.Module):
    def __init__(self):
        super(PatchDiscriminator70, self).__init__()

        # Down sampling
        self.block1 =torch.nn.Conv2d(1 + opt_out_channels, opt_start_channels, 7, 1, 3)
        # self.block1=torch.nn.leaky_relu(self.block1)
        self.block2 =torch.nn.Conv2d(opt_start_channels, opt_start_channels * 2, 3, 2, 1)
        # self.block2=torch.nn.leaky_relu(self.block2)
        self.block3 =torch.nn.Conv2d(opt_start_channels * 2, opt_start_channels * 4, 3, 2, 1)
        # self.block3=torch.nn.leaky_relu(self.block3)
        self.block4 =torch.nn.Conv2d(opt_start_channels * 4, opt_start_channels * 8, 3, 1, 1)     # stride was originally 2
        # self.block4=torch.nn.leaky_relu(self.block4)

        # Final output, implemention of 70 * 70 PatchGAN
        self.final1 =torch.nn.Conv2d(opt_start_channels * 8, opt_start_channels * 8, 4, 1, 1)
        # self.final1=torch.nn.leaky_relu(self.final1)
        self.final2 =torch.nn.Conv2d(opt_start_channels * 8, 1, 4, 1, 1)
        # self.final2=torch.nn.leaky_relu(self.final2)

    def forward(self, img_A, img_B):
        # img_A: input grayscale image
        # img_B: generated color image or ground truth color image; generated weighted image or ground truth weighted image
        # Concatenate image and condition image by channels to produce input
        
        x = torch.cat((img_A, img_B), 1)                        # out: batch * 4 * 256 * 256
        # print("after concat==>\n",x)
        # Inference
        x = F.leaky_relu (self.block1(x))                                      # out: batch * 64 * 256 * 256
        x =  F.leaky_relu (self.block2(x))                             # out: batch * 128 * 128 * 128
        x =  F.leaky_relu (self.block3(x))                            # out: batch * 256 * 64 * 64
        x = F.leaky_relu (self.block4(x))                            # out: batch * 512 * 32 * 32
        
        x = F.leaky_relu (self.final1(x))                             # out: batch * 512 * 31 * 31
        x =  F.sigmoid(self.final2(x))                             # out: batch * 1 * 30 * 30
        return x

# no need to run this cell
'''from torchsummary import summary
if __name__ == "__main__":
    net = SCGAN().cuda()
    summary(net,(1,64,64))
    
    #torch.save(net.state_dict(), 'test.pth')
    a = torch.randn(1,1, 64, 64).cuda()
    print(a.shape)
    print("a\n",a)


    print("gray_image type==>", type(gray_image) )
    a=gray_image[5].cuda()
    a_=a.reshape([1,1,64,64])   #batch,channel,h,w
    print(type(a_))
    print(a_.type())  #double tensor


    print("gray_image type==>", type(gray_image) )
    # a_=torch.from_numpy(a_)#.contiguous()
    b = net.forward(a_.float())
    # print("b=================>\n",b)
    # print(b.shape)
    # print(c.shape)

    print("\n\n\n\n")


    # loss = torch.mean(b)

    # b=b.cpu().detach().numpy() 
    # b_=b.reshape([64,64,3])   # 1,3,64,64
    # imshow(b_)
    # print(loss)
    # loss.backward()


    # print("CALLING DISCRIMINATOR...")
    # net_dis= PatchDiscriminator70().cuda()
    # c=net_dis.forward(a_,b)

    # print("o/p from discrimantor",c)
    # print(type(c))
    # print(c.shape)
    # print(c.sum())'''

# Save the model
def save_model(epoch, generator,flag):
  """Save the model at "checkpoint_interval" and its multiple"""


  if flag==0:
    model_name = 'edge_30to60_SCGAN_epoch.pth' 
    save_name = os.path.join('/content/drive/My Drive/DL project/endsem/models/', model_name)
    checkpoint = {'model': SCGAN(), 'state_dict': generator.state_dict()}
    torch.save(checkpoint,save_name)
    print('The trained model(G) is saved as %s' % (model_name))
  elif flag==1:
    model_name = 'edge_30to60_discriminator_epoch.pth'
    save_name = os.path.join('/content/drive/My Drive/DL project/endsem/models/', model_name)
    checkpoint = {'model': PatchDiscriminator70(), 'state_dict': generator.state_dict()}
    torch.save(checkpoint,save_name)
    print('The trained model(D) is saved as %s' % (model_name))

opt_lr_g=10e-4
opt_lr_d=1e-5     
opt_b1=0.9
opt_b2=0.99

opt_epochs=30

opt_lambda_l1=1
opt_lambda_gan= 0.05            
opt_lr_decrease_factor=0.5
opt_lr_decrease_epoch=10
opt_lambda_sal=0.5

# opt_lr_g=2e-5 



def trainer_WGAN(generator,discriminator):


    # Loss functions
    criterion_L1 = torch.nn.L1Loss().cuda()

    # Initialize Generator
    
    # perceptualnet = utils.create_perceptualnet(opt)

    # Optimizers
    optimizer_G = torch.optim.Adam(generator.parameters(), lr = opt_lr_g, betas = (opt_b1, opt_b2))
    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr = opt_lr_d, betas = (opt_b1, opt_b2))

     # Learning rate decrease
    def adjust_learning_rate( epoch, optimizer):
        # Set the learning rate to the initial LR decayed by "lr_decrease_factor" every "lr_decrease_epoch" epochs
    
        lr = opt_lr_g * (opt_lr_decrease_factor ** (epoch // opt_lr_decrease_epoch))
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr



    # ----------------------------------------
    #             Network dataset
    # ----------------------------------------
    # Define the dataloader
    # trainset=  list(zip(gray_image,RGB_image,sal_image))
    # dataloader = (trainset, batch_sizeDataLoader = opt.batch_size, shuffle = True, num_workers = opt.num_workers, pin_memory = True)
    # dataloader=trainset

    # ----------------------------------------
    #                 Training
    # ----------------------------------------

    # Count start time
    # prev_time = time.time()

    # For loop training

    loss_X=np.arange(opt_epochs)
    loss_Y_generator=np.array([])
    loss_Y_discriminator=np.array([])
    loss_Y_generator_lossL1=np.array([])
    loss_Y_generator_lossGAN = np.array([])
    loss_Y_generator_lossL1_sal = np.array([])
    loss_Y_discriminator_D_fake = np.array([])
    loss_Y_discriminator_D_true = np.array([])

    for epoch in range(opt_epochs):
        print("epoch",epoch)

        sum_loss_of_an_epoch_of_gen=0
        sum_loss_of_an_epoch_of_disc=0
        sum_loss_of_an_epoch_of_lossL1=0
        sum_loss_of_an_epoch_of_lossGAN=0
        sum_loss_of_an_epoch_of_lossL1_sal=0
        sum_loss_of_an_epoch_of_D_fake=0
        sum_loss_of_an_epoch_of_D_true=0

        for i, (true_stack_gray_edge, true_RGB, true_saliency) in enumerate(train_dataset):
            
            '''if (i>25000):
              break'''
            # if (i>2):
            #   break

            # if (i%5 == 0):
            #   print("i:",i)
            true_stack_gray_edge = true_stack_gray_edge.float()
            true_L = true_stack_gray_edge[:,0:1,:,:]
            # true_edge = true_stack_gray_edge[:,1:2,:,:]
            true_L = true_L.float()
            true_RGB = true_RGB.float()
            true_saliency = true_saliency.float()
            
            # true_edge = true_edge.float()
            ### Train Discriminator
            optimizer_D.zero_grad()

            # Generator output
            # print(true_stack_gray_edge.shape)
            fake_RGB,fake_saliency = generator.forward(true_stack_gray_edge)

            # Fake colorizations
            fake_scalar_d = discriminator.forward(true_L, fake_RGB.detach())
            

            # True colorizations
            true_scalar_d = discriminator.forward(true_L, true_RGB)

            loss_D_true = torch.mean(true_scalar_d)
            loss_D_fake = torch.mean(fake_scalar_d)
            # if (i%100==0):
            print("epochs =>",epoch)
            print("true D loss",loss_D_true.item() )
            print("fake scalar loss", loss_D_fake.item())
            # Overall Loss and optimize
            loss_D = - loss_D_true + loss_D_fake   # gen wants it to be 0, des -1 , range -1,1
            loss_D.backward()
            optimizer_D.step()

            ### Train Generator
            optimizer_G.zero_grad()

            # fake_RGB = generator.forward(true_L)  

            # Pixel-level L1 Loss
            loss_L1 = criterion_L1(fake_RGB, true_RGB)

            fake_stacked_saliency = torch.cat((fake_saliency,fake_saliency,fake_saliency),1)
            fake_weighted_saliency = fake_stacked_saliency.mul(fake_RGB)
            
            true_stacked_saliency = torch.cat((true_saliency,true_saliency,true_saliency),1)
            true_weighted_saliency = true_stacked_saliency.mul(true_RGB)
            
            loss_L1_sal = criterion_L1(fake_weighted_saliency, true_weighted_saliency)
            # print("fake shape",fake_RGB.shape)
            # print("true_RGB shape",true_RGB.shape)
            # print("true_L shape",true_L.shape)

            # GAN Loss
            fake_scalar = discriminator.forward(true_L, fake_RGB)
            # print("fake scalr shape:",fake_scalar.shape)
            loss_GAN = - torch.mean(fake_scalar)

            # Overall Loss and optimize
            #loss_G = opt_lambda_l1 * loss_L1 + opt_lambda_gan * loss_GAN

            # if (i%2 ==0):
              
            print("loss L1",loss_L1.item())    # 0 to 2
            print("loss_GAn",loss_GAN.item())  # -1 to 0
            print("sal loss",loss_L1_sal.item()) # 0 to 2
            loss_G = loss_L1 + opt_lambda_gan * loss_GAN + opt_lambda_sal * loss_L1_sal
            loss_G.backward()
            optimizer_G.step()


            # if i%100 == 0:
              
            print("[D Loss: %.4f] "% loss_D)
            print(" [G Loss: %.4f] "% loss_G)

            
            
            # # Learning rate decrease at certain epochs
            adjust_learning_rate( (epoch + 1),  optimizer_G)
            adjust_learning_rate( (epoch + 1),  optimizer_D)

            sum_loss_of_an_epoch_of_lossL1 +=loss_L1
            sum_loss_of_an_epoch_of_lossGAN +=loss_GAN
            sum_loss_of_an_epoch_of_lossL1_sal +=loss_L1_sal
            sum_loss_of_an_epoch_of_gen+=loss_G
            sum_loss_of_an_epoch_of_disc+=loss_D
            sum_loss_of_an_epoch_of_D_fake += loss_D_fake
            sum_loss_of_an_epoch_of_D_true += loss_D_true

        # ### Sample data every epoch
        #if (epoch+1) % 5 == 0:
            
        #     if (epoch) % 25 == 0 or (epoch== opt_epochs-1):
        #         if i%250 == 0:
          # if (i%250 ==0):
            output_display(fake_RGB,epoch,i,'fake')
            output_display(true_RGB,epoch,i,'true')

        save_model((epoch+1), generator,flag=0)
        save_model((epoch+1), discriminator,flag=1)
        

        no_of_batches=int(5000/batch_size)
        #avg_loss_of_an_epoch_of_gen=sum_loss_of_an_epoch_of_gen/len(dataloader)
        avg_loss_of_an_epoch_of_lossL1=sum_loss_of_an_epoch_of_lossL1/no_of_batches
        avg_loss_of_an_epoch_of_lossGAN=sum_loss_of_an_epoch_of_lossGAN/no_of_batches
        avg_loss_of_an_epoch_of_lossL1_sal=sum_loss_of_an_epoch_of_lossL1_sal/no_of_batches
        avg_loss_of_an_epoch_of_gen=sum_loss_of_an_epoch_of_gen/no_of_batches
        avg_loss_of_an_epoch_of_D_fake=sum_loss_of_an_epoch_of_D_fake/no_of_batches
        avg_loss_of_an_epoch_of_D_true=sum_loss_of_an_epoch_of_D_true/no_of_batches
        #avg_loss_of_an_epoch_of_disc=sum_loss_of_an_epoch_of_disc/len(dataloader)
        avg_loss_of_an_epoch_of_disc=sum_loss_of_an_epoch_of_disc/no_of_batches
        
        loss_Y_generator=np.append(loss_Y_generator, avg_loss_of_an_epoch_of_gen.cpu().detach().numpy())
        loss_Y_discriminator=np.append(loss_Y_discriminator,avg_loss_of_an_epoch_of_disc.cpu().detach().numpy() )

        loss_Y_generator_lossL1=np.append(loss_Y_generator_lossL1, avg_loss_of_an_epoch_of_lossL1.cpu().detach().numpy())
        loss_Y_generator_lossGAN=np.append(loss_Y_generator_lossGAN, avg_loss_of_an_epoch_of_lossGAN.cpu().detach().numpy())
        loss_Y_generator_lossL1_sal=np.append(loss_Y_generator_lossL1_sal,avg_loss_of_an_epoch_of_lossL1_sal.cpu().detach().numpy())
        loss_Y_discriminator_D_fake=np.append(loss_Y_discriminator_D_fake,avg_loss_of_an_epoch_of_D_fake.cpu().detach().numpy())
        loss_Y_discriminator_D_true=np.append(loss_Y_discriminator_D_true,avg_loss_of_an_epoch_of_D_true.cpu().detach().numpy())


        # joblib.dump(loss_X, '/content/drive/My Drive/DL project/endsem/models/loss_X_run1 ')
        joblib.dump(loss_Y_generator,'/content/drive/My Drive/DL project/endsem/models/loss_Y_generator_run2')
        joblib.dump(loss_Y_discriminator, '/content/drive/My Drive/DL project/endsem/models/loss_Y_discriminator_run2')
        joblib.dump(loss_Y_generator_lossL1, '/content/drive/My Drive/DL project/endsem/models/loss_Y_generator_lossL1_run2')
        joblib.dump(loss_Y_generator_lossGAN, '/content/drive/My Drive/DL project/endsem/models/loss_Y_generator_lossGAN_run2')
        joblib.dump(loss_Y_generator_lossL1_sal, '/content/drive/My Drive/DL project/endsem/models/loss_Y_generator_lossL1_sal_run2')
        joblib.dump(loss_Y_discriminator_D_fake, '/content/drive/My Drive/DL project/endsem/models/loss_Y_discriminator_D_fake_run2')
        joblib.dump(loss_Y_discriminator_D_true, '/content/drive/My Drive/DL project/endsem/models/loss_Y_discriminator_D_true_run2')
        

    return loss_X, loss_Y_generator, loss_Y_discriminator,loss_Y_generator_lossL1,loss_Y_generator_lossGAN,loss_Y_generator_lossL1_sal,\
    loss_Y_discriminator_D_fake,loss_Y_discriminator_D_true,generator,discriminator,optimizer_G





# generator = SCGAN().cuda()
# generator.apply(weights_init)
# discriminator = PatchDiscriminator70().cuda()

generator_name = '/content/drive/My Drive/DL project/endsem/models/edge_0to30_SCGAN_epoch.pth'
discriminator_name = '/content/drive/My Drive/DL project/endsem/models/edge_0to30_discriminator_epoch.pth'

checkpoint = torch.load(generator_name)
generator = checkpoint['model']
generator.load_state_dict(checkpoint['state_dict'])
generator=generator.cuda()

checkpoint1 = torch.load(discriminator_name)
discriminator = checkpoint1['model']
discriminator.load_state_dict(checkpoint1['state_dict'])
discriminator=discriminator.cuda()

loss_X, loss_Y_generator, loss_Y_discriminator,loss_Y_generator_lossL1,loss_Y_generator_lossGAN,loss_Y_generator_lossL1_sal,\
    loss_Y_discriminator_D_fake,loss_Y_discriminator_D_true,generator,discriminator,optimizer_G = trainer_WGAN(generator,discriminator)

'''
def save_loss_generator_and_disc_and_model(loss_X, loss_Y_generator,  loss_Y_discriminator ,generator,optimizer_G):
    joblib.dump(loss_X, '/content/drive/My Drive/DL project/endsem/models/loss_X ')
    joblib.dump(loss_Y_generator,'/content/drive/My Drive/DL project/endsem/models/loss_Y_generator')
    joblib.dump(loss_Y_discriminator, '/content/drive/My Drive/DL project/endsem/models/loss_Y_discriminator')
    joblib.dump(loss_Y_generator_lossL1, '/content/drive/My Drive/DL project/endsem/models/loss_Y_generator_lossL1')
    joblib.dump(loss_Y_generator_lossGAN, '/content/drive/My Drive/DL project/endsem/models/loss_Y_generator_lossGAN')
    joblib.dump(loss_Y_generator_lossL1_sal, '/content/drive/My Drive/DL project/endsem/models/loss_Y_generator_lossL1_sal')
    joblib.dump(loss_Y_discriminator_D_fake, '/content/drive/My Drive/DL project/endsem/models/loss_Y_discriminator_D_fake')
    joblib.dump(loss_Y_discriminator_D_true, '/content/drive/My Drive/DL project/endsem/models/loss_Y_discriminator_D_true')
    

    # print("The state dict keys: \n\n", generator.state_dict().keys())
    # checkpoint = {'model': SCGAN(),
    #           'state_dict': generator.state_dict(),
    #           'optimizer' : optimizer_G.state_dict()}

    # torch.save(checkpoint, '/content/drive/My Drive/DL project/endsem/models/checkpoint.pth')

save_loss_generator_and_disc_and_model(loss_X, loss_Y_generator,  loss_Y_discriminator ,generator,optimizer_G)  
'''

import matplotlib.pyplot as plt

def plot():

    loss_Y_generator =joblib.load('/content/drive/My Drive/DL project/endsem/models/loss_Y_generator1')
    loss_Y_discriminator=joblib.load('/content/drive/My Drive/DL project/endsem/models/loss_Y_discriminator1')
    loss_Y_generator_lossL1=joblib.load('/content/drive/My Drive/DL project/endsem/models/loss_Y_generator_lossL11')
    loss_Y_generator_lossGAN=joblib.load('/content/drive/My Drive/DL project/endsem/models/loss_Y_generator_lossGAN1')
    loss_Y_generator_lossL1_sal=joblib.load('/content/drive/My Drive/DL project/endsem/models/loss_Y_generator_lossL1_sal1')
    loss_Y_discriminator_D_fake=joblib.load('/content/drive/My Drive/DL project/endsem/models/loss_Y_discriminator_D_fake1')
    loss_Y_discriminator_D_true=joblib.load('/content/drive/My Drive/DL project/endsem/models/loss_Y_discriminator_D_true1')
    
    


    loss_X = np.arange(1,61)

    plt.plot(loss_X, loss_Y_generator, color="red", label="overall generator loss")
    plt.plot(loss_X, loss_Y_generator_lossL1, color="orange",  label="generator loss L1")
    plt.plot(loss_X, loss_Y_generator_lossGAN, color="green",  label="GAN loss")
    plt.plot(loss_X, loss_Y_generator_lossL1_sal, color="yellow",  label="generator loss L1 saliency")
    # plt.plot(loss_X, loss_Y_discriminator_D_fake, color="cyan",  label="discriminator loss fake")
    # plt.plot(loss_X, loss_Y_discriminator_D_true, color="black",  label="discriminator loss true")
    # plt.plot(loss_X, loss_Y_discriminator, color="blue",  label="overall discriminator loss")


    plt.xlabel("epoch")
    plt.ylabel("loss")

    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))


plot()

def plot2():

    # loss_Y_generator =joblib.load('/content/drive/My Drive/DL project/endsem/models/loss_Y_generator1')
    loss_Y_discriminator=joblib.load('/content/drive/My Drive/DL project/endsem/models/loss_Y_discriminator1')
    # loss_Y_generator_lossL1=joblib.load('/content/drive/My Drive/DL project/endsem/models/loss_Y_generator_lossL11')
    # loss_Y_generator_lossGAN=joblib.load('/content/drive/My Drive/DL project/endsem/models/loss_Y_generator_lossGAN1')
    # loss_Y_generator_lossL1_sal=joblib.load('/content/drive/My Drive/DL project/endsem/models/loss_Y_generator_lossL1_sal1')
    loss_Y_discriminator_D_fake=joblib.load('/content/drive/My Drive/DL project/endsem/models/loss_Y_discriminator_D_fake1')
    loss_Y_discriminator_D_true=joblib.load('/content/drive/My Drive/DL project/endsem/models/loss_Y_discriminator_D_true1')
    
    


    loss_X = np.arange(1,61)

    # plt.plot(loss_X, loss_Y_generator, color="red", label="overall generator loss")
    # plt.plot(loss_X, loss_Y_generator_lossL1, color="orange",  label="generator loss L1")
    # plt.plot(loss_X, loss_Y_generator_lossGAN, color="green",  label="GAN loss")
    # plt.plot(loss_X, loss_Y_generator_lossL1_sal, color="yellow",  label="generator loss L1 saliency")
    plt.plot(loss_X, loss_Y_discriminator_D_fake, color="cyan",  label="discriminator loss fake")
    plt.plot(loss_X, loss_Y_discriminator_D_true, color="black",  label="discriminator loss true")
    plt.plot(loss_X, loss_Y_discriminator, color="blue",  label="overall discriminator loss")


    plt.xlabel("epoch")
    plt.ylabel("loss")

    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))


plot2()

from math import log10, sqrt
import cv2
import numpy as np
from skimage.metrics import structural_similarity as ssim
  
def PSNR(original, compressed):
    if (torch.mean((original - compressed) ** 2) == 0):  
        return 100
    max_pixel = 255.0
    psnr = ((20 * log10(max_pixel / sqrt(torch.mean((original - compressed) ** 2))))-44)
    return psnr

# Commented out IPython magic to ensure Python compatibility.
# ls '/content/drive/MyDrive/flickr30k_images/flickr30k_images' | wc -l
# %cd /content/drive/MyDrive/flickr30k_images/flickr30k_images
# !pwd
# !ls | wc -l
!ls '/content/drive/MyDrive/flickr30k_images/flickr30k_images' | wc -l

generator_name = '/content/drive/My Drive/DL project/endsem/models2/final_model.pth'
# discriminator_name = '/content/drive/My Drive/DL project/endsem/models/final_discriminator.pth'

checkpoint = torch.load(generator_name)
generator = checkpoint['model']
generator.load_state_dict(checkpoint['state_dict'])
generator=generator.cuda()

# checkpoint1 = torch.load(discriminator_name)
# discriminator = checkpoint1['model']
# discriminator.load_state_dict(checkpoint1['state_dict'])
# discriminator=discriminator.cuda()

def test_load_preprocess():
  folder_name = '/content/drive/MyDrive/flickr30k_images/flickr30k_images'
  cnt=0
  # folder_name = '/content/drive/MyDrive/DL project/endsem/test'
  RGB_image_list=[]
  # gray_image_list=[]
  sal_image_list=[]
  stack_gray_edge_list=[]
  for file in os.listdir(folder_name):
    if file[-1]!='g':
      print(file)
      continue
    cnt=cnt+1
    print(cnt)
    if cnt<6000:
      continue
    if cnt>11000:
      break

  
  
    file = cv2.imread(folder_name + '/' + file)
    # print(file)
    file = cv2.resize(file,(64,64))
    gray_image=cv2.cvtColor(file,cv2.COLOR_BGR2GRAY)
    edge_image = cv2.Canny(gray_image,100,200,apertureSize=3,L2gradient = True)

    img = (file-128.0)/128.0
    gray_image = (gray_image-128.0)/128.0
    edge_image = edge_image/255
    
    
    saliency = cv2.saliency.StaticSaliencyFineGrained_create()
    (success, saliencyMap) = saliency.computeSaliency(file)
    saliencyMap_tanh = (saliencyMap*2)-1
    saliencyMap_tanh = torch.from_numpy(saliencyMap_tanh)
    # print(saliencyMap_tanh.shape)

    grayimg = torch.from_numpy(gray_image)
    edgeimg = torch.from_numpy(edge_image)
    img = torch.from_numpy(img)
    img = img.permute(2,0,1)
    img = img.reshape(1,img.shape[0],img.shape[1],img.shape[2])
    
    grayimg = grayimg.reshape(1,1,grayimg.shape[0],grayimg.shape[1])
    edgeimg = edgeimg.reshape(1,1,edgeimg.shape[0],edgeimg.shape[1])
    stack_gray_edge = torch.cat((grayimg,edgeimg),1)

    saliencyMap_tanh = saliencyMap_tanh.reshape(1,1,saliencyMap_tanh.shape[0],saliencyMap_tanh.shape[1])

    RGB_image_list.append(img)
    # gray_image_list.append(grayimg)
    stack_gray_edge_list.append(stack_gray_edge)
    sal_image_list.append(saliencyMap_tanh)
  # print(len(RGB_image_list))
  # print(len(gray_image_list))
  return RGB_image_list,stack_gray_edge_list,sal_image_list

# test_RGB_image_list,test_stack_gray_edge_list,test_sal_image_list = test_load_preprocess()

print(test_RGB_image_list[0].shape)
print(test_stack_gray_edge_list[0].shape)
print(test_sal_image_list[0].shape)

# joblib.dump(test_RGB_image_list,'/content/drive/My Drive/DL project/endsem/test_preprocess/test_RGB_image_list')
# joblib.dump(test_stack_gray_edge_list,'/content/drive/My Drive/DL project/endsem/test_preprocess/test_stack_gray_edge_list')
# joblib.dump(test_sal_image_list,'/content/drive/My Drive/DL project/endsem/test_preprocess/test_sal_image_list')

test_RGB_image_list = joblib.load('/content/drive/My Drive/DL project/endsem/test_preprocess/test_RGB_image_list')
test_stack_gray_edge_list = joblib.load('/content/drive/My Drive/DL project/endsem/test_preprocess/test_stack_gray_edge_list')
test_sal_image_list = joblib.load('/content/drive/My Drive/DL project/endsem/test_preprocess/test_sal_image_list')

#                       test

# Loss functions
# criterion = torch.nn.MSELoss().cuda()

generator.eval()
test_loss=0
avg_psnr=0
for i in range (len(test_stack_gray_edge_list)):
    test_stack_gray_edge_list[i]= test_stack_gray_edge_list[i].float().cuda()
    test_RGB_image_list[i]= test_RGB_image_list[i].float().cuda()
    fake_rgb,fake_saliency = generator.forward(test_stack_gray_edge_list[i])
    # fake_RGB_by_gan_from_True_L_test=fake_RGB_by_gan_from_True_L_test.reshape(1,3,64,64)

    #generator=load_loss_generator_and_disc_and_model()   #SCGAN().cuda()
    
    # test_loss += criterion(fake_RGB_by_gan_from_True_L_test, True_rgb_test) #.item()

    # cv2_imshow(True_L_test)
    # cv2_imshow(fake_RGB_by_gan_from_True_L_test)
    psnr_val = PSNR(test_RGB_image_list[i],fake_rgb)
    avg_psnr +=psnr_val
    if psnr_val > 21:
      print("PSNR",psnr_val)
      output_display_test(test_stack_gray_edge_list[i][:,0:1,:,:],0,i, 'gray')
      output_display_test(fake_rgb,1,i,'fake')
      # output_display_test(test_RGB_image_list[i],1,i,'real')

    
    # val_true_rgb = test_RGB_image_list[i][0].permute(1,2,0)
    # val_fake_rgb = fake_rgb[0].permute(1,2,0)
    # val_true_rgb = val_true_rgb.detach().cpu().numpy()
    # val_fake_rgb = val_fake_rgb.detach().cpu().numpy()
    # print(type(val_true_rgb))
    # print(type(val_fake_rgb))
    # val_true_rgb = np.zeros((64,64,3),dtype='uint8')
    # val_fake_rgb = np.zeros((64,64,3),dtype='uint8')
    # ssim = ssim(val_true_rgb,val_fake_rgb,data_range=val_fake_rgb.max()-val_fake_rgb.min())
    # print("ssim", ssim)

    #  print(f"PSNR value is {value} dB")

    

    # print("\n\n")

# avg_test_loss=test_loss/len(gray_image_test)
# print("Avg. test loss",avg_test_loss)
avg_psnr = avg_psnr/len(test_stack_gray_edge_list)

print("average psnr",avg_psnr)

def test_load_preprocess():
  # folder_name = '/content/drive/MyDrive/flickr30k_images/flickr30k_images'
  cnt=0
  folder_name = '/content/drive/MyDrive/DL project/endsem/test'
  RGB_image_list=[]
  # gray_image_list=[]
  sal_image_list=[]
  stack_gray_edge_list=[]
  
  for file in os.listdir(folder_name):
    file = cv2.imread(folder_name + '/' + file)
    # print(file)
    file = cv2.resize(file,(64,64))
    gray_image=cv2.cvtColor(file,cv2.COLOR_BGR2GRAY)
    edge_image = cv2.Canny(gray_image,100,200,apertureSize=3,L2gradient = True)

    img = (file-128.0)/128.0
    gray_image = (gray_image-128.0)/128.0
    edge_image = edge_image/255
    
    
    saliency = cv2.saliency.StaticSaliencyFineGrained_create()
    (success, saliencyMap) = saliency.computeSaliency(file)
    saliencyMap_tanh = (saliencyMap*2)-1
    saliencyMap_tanh = torch.from_numpy(saliencyMap_tanh)
    # print(saliencyMap_tanh.shape)

    grayimg = torch.from_numpy(gray_image)
    edgeimg = torch.from_numpy(edge_image)
    img = torch.from_numpy(img)
    img = img.permute(2,0,1)
    img = img.reshape(1,img.shape[0],img.shape[1],img.shape[2])
    
    grayimg = grayimg.reshape(1,1,grayimg.shape[0],grayimg.shape[1])
    edgeimg = edgeimg.reshape(1,1,edgeimg.shape[0],edgeimg.shape[1])
    stack_gray_edge = torch.cat((grayimg,edgeimg),1)

    saliencyMap_tanh = saliencyMap_tanh.reshape(1,1,saliencyMap_tanh.shape[0],saliencyMap_tanh.shape[1])

    RGB_image_list.append(img)
    # gray_image_list.append(grayimg)
    stack_gray_edge_list.append(stack_gray_edge)
    sal_image_list.append(saliencyMap_tanh)
  # print(len(RGB_image_list))
  # print(len(gray_image_list))
  return RGB_image_list,stack_gray_edge_list,sal_image_list

test_RGB_image_list1,test_stack_gray_edge_list1,test_sal_image_list1 = test_load_preprocess()

print(test_RGB_image_list1[0].shape)
print(test_stack_gray_edge_list1[0].shape)
print(test_sal_image_list1[0].shape)

generator_name = '/content/drive/My Drive/DL project/endsem/models2/final_model.pth'
# discriminator_name = '/content/drive/My Drive/DL project/endsem/models/final_discriminator.pth'

checkpoint = torch.load(generator_name)
generator = checkpoint['model']
generator.load_state_dict(checkpoint['state_dict'])
generator=generator.cuda()
generator.eval()
test_loss=0
avg_psnr1=0
for i in range (len(test_stack_gray_edge_list1)):
    test_stack_gray_edge_list1[i]= test_stack_gray_edge_list1[i].float().cuda()
    test_RGB_image_list1[i]= test_RGB_image_list1[i].float().cuda()
    fake_rgb1,fake_saliency1 = generator.forward(test_stack_gray_edge_list1[i])
    # fake_RGB_by_gan_from_True_L_test=fake_RGB_by_gan_from_True_L_test.reshape(1,3,64,64)

    #generator=load_loss_generator_and_disc_and_model()   #SCGAN().cuda()
    
    # test_loss += criterion(fake_RGB_by_gan_from_True_L_test, True_rgb_test) #.item()

    # cv2_imshow(True_L_test)
    # cv2_imshow(fake_RGB_by_gan_from_True_L_test)
    psnr_val1 = PSNR(test_RGB_image_list1[i],fake_rgb1)
    # if psnr_val1 > 21:
    print("PSNR",psnr_val1)
    avg_psnr1 += psnr_val1
    output_display_test(test_stack_gray_edge_list1[i][:,0:1,:,:],0,i, 'gray')
    output_display_test(fake_rgb1,1,i,'fake')
    # output_display_test(test_RGB_image_list[i],1,i,'real')

    
    # val_true_rgb = test_RGB_image_list[i][0].permute(1,2,0)
    # val_fake_rgb = fake_rgb[0].permute(1,2,0)
    # val_true_rgb = val_true_rgb.detach().cpu().numpy()
    # val_fake_rgb = val_fake_rgb.detach().cpu().numpy()
    # print(type(val_true_rgb))
    # print(type(val_fake_rgb))
    # val_true_rgb = np.zeros((64,64,3),dtype='uint8')
    # val_fake_rgb = np.zeros((64,64,3),dtype='uint8')
    # ssim = ssim(val_true_rgb,val_fake_rgb,data_range=val_fake_rgb.max()-val_fake_rgb.min())
    # print("ssim", ssim)

    #  print(f"PSNR value is {value} dB")
print("average psnr",avg_psnr1/len(test_stack_gray_edge_list1))
    

    # print("\n\n")

# avg_test_loss=test_loss/len(gray_image_test)
# print("Avg. test loss",avg_test_loss)

def custom_load_preprocess():
  # folder_name = '/content/drive/MyDrive/flickr30k_images/flickr30k_images'
  cnt=0
  folder_name = '/content/drive/MyDrive/DL project/endsem/custom test'
  RGB_image_list=[]
  # gray_image_list=[]
  sal_image_list=[]
  stack_gray_edge_list=[]
  
  for file in os.listdir(folder_name):
    file = cv2.imread(folder_name + '/' + file)
    # print(file)
    file = cv2.resize(file,(64,64))
    gray_image=cv2.cvtColor(file,cv2.COLOR_BGR2GRAY)
    edge_image = cv2.Canny(gray_image,100,200,apertureSize=3,L2gradient = True)

    img = (file-128.0)/128.0
    gray_image = (gray_image-128.0)/128.0
    edge_image = edge_image/255
    
    
    saliency = cv2.saliency.StaticSaliencyFineGrained_create()
    (success, saliencyMap) = saliency.computeSaliency(file)
    saliencyMap_tanh = (saliencyMap*2)-1
    saliencyMap_tanh = torch.from_numpy(saliencyMap_tanh)
    # print(saliencyMap_tanh.shape)

    grayimg = torch.from_numpy(gray_image)
    edgeimg = torch.from_numpy(edge_image)
    img = torch.from_numpy(img)
    img = img.permute(2,0,1)
    img = img.reshape(1,img.shape[0],img.shape[1],img.shape[2])
    
    grayimg = grayimg.reshape(1,1,grayimg.shape[0],grayimg.shape[1])
    edgeimg = edgeimg.reshape(1,1,edgeimg.shape[0],edgeimg.shape[1])
    stack_gray_edge = torch.cat((grayimg,edgeimg),1)

    saliencyMap_tanh = saliencyMap_tanh.reshape(1,1,saliencyMap_tanh.shape[0],saliencyMap_tanh.shape[1])

    RGB_image_list.append(img)
    # gray_image_list.append(grayimg)
    stack_gray_edge_list.append(stack_gray_edge)
    sal_image_list.append(saliencyMap_tanh)
  # print(len(RGB_image_list))
  # print(len(gray_image_list))
  return RGB_image_list,stack_gray_edge_list,sal_image_list

test_RGB_image_list1,test_stack_gray_edge_list1,sal_image_list = custom_load_preprocess()

print(test_RGB_image_list1[0].shape)
print(test_stack_gray_edge_list1[0].shape)

generator_name = '/content/drive/My Drive/DL project/endsem/models2/final_model.pth'
# discriminator_name = '/content/drive/My Drive/DL project/endsem/models/final_discriminator.pth'

checkpoint = torch.load(generator_name)
generator = checkpoint['model']
generator.load_state_dict(checkpoint['state_dict'])
generator=generator.cuda()
generator.eval()
# test_loss=0
# avg_psnr1=0
for i in range (len(test_stack_gray_edge_list1)):
    test_stack_gray_edge_list1[i]= test_stack_gray_edge_list1[i].float().cuda()
    test_RGB_image_list1[i]= test_RGB_image_list1[i].float().cuda()
    fake_rgb1,fake_saliency1 = generator.forward(test_stack_gray_edge_list1[i])
    # fake_RGB_by_gan_from_True_L_test=fake_RGB_by_gan_from_True_L_test.reshape(1,3,64,64)

    #generator=load_loss_generator_and_disc_and_model()   #SCGAN().cuda()
    
    # test_loss += criterion(fake_RGB_by_gan_from_True_L_test, True_rgb_test) #.item()

    # cv2_imshow(True_L_test)
    # cv2_imshow(fake_RGB_by_gan_from_True_L_test)
    # psnr_val1 = PSNR(test_RGB_image_list1[i],fake_rgb1)
    # if psnr_val1 > 21:


    # print("PSNR",psnr_val1)
    # avg_psnr1 += psnr_val1
    output_display_test(test_stack_gray_edge_list1[i][:,0:1,:,:],0,i, 'gray')
    output_display_test(fake_rgb1,1,i,'fake')
    # output_display_test(test_RGB_image_list[i],1,i,'real')

    
    # val_true_rgb = test_RGB_image_list[i][0].permute(1,2,0)
    # val_fake_rgb = fake_rgb[0].permute(1,2,0)
    # val_true_rgb = val_true_rgb.detach().cpu().numpy()
    # val_fake_rgb = val_fake_rgb.detach().cpu().numpy()
    # print(type(val_true_rgb))
    # print(type(val_fake_rgb))
    # val_true_rgb = np.zeros((64,64,3),dtype='uint8')
    # val_fake_rgb = np.zeros((64,64,3),dtype='uint8')
    # ssim = ssim(val_true_rgb,val_fake_rgb,data_range=val_fake_rgb.max()-val_fake_rgb.min())
    # print("ssim", ssim)

    #  print(f"PSNR value is {value} dB")


# print("average psnr",avg_psnr1/len(test_stack_gray_edge_list1))

